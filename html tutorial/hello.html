<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>print hello</title>
</head>
<body>
    <h1>1 computer architecture </h1>
    <p>Computer architecture involves balancing various goals, such as cost, speed, availability, and energy efficiency. Designers must have a thorough understanding of hardware requirements and diverse aspects of computing, ranging from compilers to Integrated circuit design. [14] Cost has also become a significant constraint for manufacturers seeking to sell their products for less money than competitors offering a very similar hardware component. Profit margins have also been reduced.[15] Even when the performance is not increasing, the cost of components has been dropping over time due to improved manufacturing techniques that have fewer components rejected at quality assurance stage.[16]

Instruction set architecture
The most common instruction set architecture (ISA)—the interface between a computer's hardware and software—is based on the one devised by von Neumann in 1945.[17] Despite the separation of the computing unit and the I/O system in many diagrams, typically the hardware is shared, with a bit in the computing unit indicating whether it is in computation or I/O mode.[18] Common types of ISAs include CISC (complex instruction set computer), RISC (reduced instruction set computer), vector operations, and hybrid modes.[19] CISC involves using a larger expression set to minimize the number of instructions the machines need to use.[20] Based on a recognition that only a few instructions are commonly used, RISC shrinks the instruction set for added simplicity, which also enables the inclusion of more registers.[21] After the invention of RISC in the 1980s, RISC based architectures that used pipelining and caching to increase performance displaced CISC architectures, particularly in applications with restrictions on power usage or space (such as mobile phones). From 1986 to 2003, the annual rate of improvement in hardware performance exceeded 50 percent, enabling the development of new computing devices such as tablets and mobiles.[22] Alongside the density of transistors, DRAM memory as well as flash and magnetic disk storage also became exponentially more compact and cheaper. The rate of improvement slackened off in the twenty-first century.[23]

In the twenty-first century, increases in performance have been driven by increasing exploitation of parallelism.[24] Applications are often parallelizable in two ways: either the same function is running across multiple areas of data (data parallelism) or different tasks can be performed simultaneously with limited interaction (task parallelism).[25] These forms of parallelism are accommodated by various hardware strategies, including instruction-level parallelism (such as instruction pipelining), vector architectures and graphical processing units (GPUs) that are able to implement data parallelism, thread-level parallelism and request-level parallelism (both implementing task-level parallelism).[25]

Microarchitecture
Microarchitecture, also known as computer organization, refers to high-level hardware questions such as the design of the CPU, memory, and memory interconnect.[26] Memory hierarchy ensures that the memory quicker to access (and more expensive) is located closer to the CPU, while slower, cheaper memory for large-volume storage is located further away.[27] Memory is typically segregated to separate programs from data and limit an attacker's ability to alter programs.[28] Most computers use virtual memory to simplify addressing for programs, using the operating system to map virtual memory to different areas of the finite physical memory.[29]

Cooling
Computer processors generate heat, and excessive heat impacts their performance and can harm the components. Many computer chips will automatically throttle their performance to avoid overheating. Computers also typically have mechanisms for dissipating excessive heat, such as air or liquid coolers for the CPU and GPU and heatsinks for other components, such as the RAM. Computer cases are also often ventilated to help dissipate heat from the computer.[30] Data centers typically use more sophisticated cooling solutions to keep the operating temperature of the entire center safe. Air-cooled systems are more common in smaller or older data centers, while liquid-cooled immersion (where each computer is surrounded by cooling fluid) and direct-to-chip (where the cooling fluid is directed to each computer chip) can be more expensive but are also more efficient.[31] Most computers are designed to be more powerful than their cooling system, but their sustained operations cannot exceed the capacity of the cooling system.[32] While performance can be temporarily increased when the computer is not hot (overclocking),[33] in order to protect the hardware from excessive heat, the system will automatically reduce performance or shut down the processor if necessary.[32] Processors also will shut off or enter a low power mode when inactive to reduce heat.[34] Power delivery as well as heat dissipation are the most challenging aspects of hardware design,[35] and have been the limiting factor to the development of smaller and faster chips since the early twenty-first century.[34] Increases in performance require a commensurate increase in energy use and cooling demand</p>

    <br>
    <a href="/aman.html">next</a>
</body>
</html>